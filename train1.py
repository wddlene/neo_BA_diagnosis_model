import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, brier_score_loss, matthews_corrcoef
from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, cross_val_predict, KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import calibration_curve
import math
import warnings
import joblib 
import itertools
from scipy.stats import gaussian_kde
import os

# ====================
# ä¸“ä¸šå¯è§†åŒ–è®¾ç½®ï¼ˆä»BATL_GBMMP3neo.ipynbå¯¼å…¥ï¼‰
# ====================
# è®¾ç½®å…¨å±€æ ·å¼
plt.style.use('seaborn-v0_8-darkgrid')

# å®šä¹‰ä¸“ä¸šè‰²å½©æ–¹æ¡ˆ
CUSTOM_COLORS = {
    'primary': '#2E86AB',    # æ·±è“è‰² - ä¸»è‰²
    'secondary': '#A23B72',  # ç´«è‰² - æ¬¡è¦è‰²
    'tertiary': '#F18F01',   # æ©™è‰² - ç¬¬ä¸‰è‰²
    'accent': '#C73E1D',     # çº¢è‰² - å¼ºè°ƒè‰²
    'success': '#2A9D8F',    # ç»¿è‰² - æˆåŠŸ
    'warning': '#E9C46A',    # é»„è‰² - è­¦å‘Š
    'dark': '#264653',       # æ·±è‰² - èƒŒæ™¯
    'light': '#E9ECEF',      # æµ…è‰² - èƒŒæ™¯
    'gray1': '#6C757D',      # ç°è‰²1
    'gray2': '#ADB5BD',      # ç°è‰²2
    'blue_gradient': ['#1A2980', '#26D0CE'],  # è“è‰²æ¸å˜
    'red_gradient': ['#FF416C', '#FF4B2B'],   # çº¢è‰²æ¸å˜
    'green_gradient': ['#11998E', '#38EF7D'], # ç»¿è‰²æ¸å˜
}

# è®¾ç½®matplotlibå…¨å±€å‚æ•°
plt.rcParams.update({
    'figure.figsize': (14, 10),
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    'font.size': 11,
    'font.family': 'sans-serif',
    'axes.labelsize': 12,
    'axes.titlesize': 14,
    'axes.titleweight': 'bold',
    'axes.linewidth': 1.2,
    'axes.grid': True,
    'grid.alpha': 0.3,
    'grid.linestyle': '--',
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'legend.frameon': True,
    'legend.framealpha': 0.9,
    'legend.edgecolor': '0.8',
    'figure.titlesize': 16,
    'figure.titleweight': 'bold',
})

# è®¾ç½®seabornæ ·å¼
sns.set_palette("husl")
sns.set_context("notebook", font_scale=1.1)
sns.set_style("whitegrid", {
    'grid.linestyle': ':',
    'grid.alpha': 0.2,
    'axes.edgecolor': '0.3',
    'axes.linewidth': 1.1,
})

# åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ˜ å°„
blue_cmap = sns.light_palette(CUSTOM_COLORS['primary'], as_cmap=True)
red_cmap = sns.light_palette(CUSTOM_COLORS['accent'], as_cmap=True)
diverging_cmap = sns.diverging_palette(240, 10, as_cmap=True)

print("âœ… Visualization settings completed")

# åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨ä¸notebookç›¸åŒçš„ç‰¹å¾ï¼‰
print("ğŸ“Š Loading neonatal biliary atresia data...")
data = pd.read_csv("BAGBMMP1209_nonscaled.csv")

# æå–ç›®æ ‡å˜é‡å’Œç‰¹å¾ - ä½¿ç”¨ç›¸åŒçš„5ä¸ªç‰¹å¾
data_target = data['BA']
data_features = data[['GB_length', 'Abnormal_GEI', 'GGT', 'DBIL', 'MMP7']]

# æ•°æ®æ ‡å‡†åŒ–
print("ğŸ”§ Standardizing features...")
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = pd.DataFrame(scaler.fit_transform(data_features), columns=data_features.columns)

# åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†ï¼ˆä¸notebookç›¸åŒçš„å‚æ•°ï¼‰
print("âœ‚ï¸ Splitting training and test sets...")
class_x_train, class_x_test, class_y_train, class_y_test = train_test_split(
    data_scaled, data_target, test_size=0.3, random_state=42, stratify=data_target
)

# ç¡®ä¿æ ‡ç­¾æ˜¯numpyæ•°ç»„æ ¼å¼
class_y_train = class_y_train.values.ravel()
class_y_test = class_y_test.values.ravel()

# æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
print(f"ğŸ“ˆ Dataset information:")
print(f"  Total samples: {data.shape[0]}")
print(f"  Number of features: {data_scaled.shape[1]}")
print(f"  Training set size: {class_x_train.shape[0]} samples")
print(f"  Test set size: {class_x_test.shape[0]} samples")
print(f"  Training class distribution: {pd.Series(class_y_train).value_counts().to_dict()}")
print(f"  Test class distribution: {pd.Series(class_y_test).value_counts().to_dict()}")

# è®¡ç®—AUCç½®ä¿¡åŒºé—´å‡½æ•°
def calculate_auc_ci(y_true, y_pred, n_bootstraps=2000, alpha=0.95):
    """Calculate AUC confidence intervals using bootstrap method"""
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()
    
    if len(np.unique(y_true)) < 2:
        return 0.5, (0.0, 1.0)
    
    n = len(y_true)
    bootstrapped_auc = []
    original_auc = roc_auc_score(y_true, y_pred)
    
    # Bootstrap sampling
    for _ in range(n_bootstraps):
        indices = np.random.choice(range(n), n, replace=True)
        if len(np.unique(y_true[indices])) < 2:
            continue
        
        auc_val = roc_auc_score(y_true[indices], y_pred[indices])
        bootstrapped_auc.append(auc_val)
    
    if len(bootstrapped_auc) == 0:
        return original_auc, (0.0, 1.0)
    
    # Calculate percentile confidence intervals
    sorted_auc = np.sort(bootstrapped_auc)
    lower_idx = int(n_bootstraps * (1 - alpha) / 2)
    upper_idx = int(n_bootstraps * (1 + alpha) / 2)
    
    ci_lower = sorted_auc[lower_idx] if lower_idx < len(sorted_auc) else sorted_auc[0]
    ci_upper = sorted_auc[upper_idx] if upper_idx < len(sorted_auc) else sorted_auc[-1]
    
    return original_auc, (ci_lower, ci_upper)

# ====================
# ä»BATL_GBMMP3neo.ipynbå¯¼å…¥çš„å¢å¼ºå¯è§†åŒ–å‡½æ•°
# ====================
def create_modern_roc_curve(y_true, y_pred_proba, set_name="", ax=None):
    """
    Create modern ROC curve
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6))
    
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    auc_val, (ci_lower, ci_upper) = calculate_auc_ci(y_true, y_pred_proba)
    
    # Main ROC curve
    ax.plot(fpr, tpr, color=CUSTOM_COLORS['primary'], 
            linewidth=3, alpha=0.9,
            label=f'AUC = {auc_val:.3f} (95% CI: {ci_lower:.3f}-{ci_upper:.3f})')
    
    # Fill confidence interval
    ax.fill_between(fpr, tpr, alpha=0.2, color=CUSTOM_COLORS['primary'])
    
    # Diagonal line
    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1.5, label='Random classifier')
    
    # Find optimal threshold (Youden's J statistic)
    j_scores = tpr - fpr
    optimal_idx = np.argmax(j_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    # Mark optimal point
    ax.scatter(fpr[optimal_idx], tpr[optimal_idx], 
               s=150, zorder=5, color=CUSTOM_COLORS['accent'],
               edgecolors='white', linewidth=2,
               label=f'Optimal point (J={j_scores[optimal_idx]:.3f})')
    
    ax.set_xlim([-0.02, 1.02])
    ax.set_ylim([-0.02, 1.02])
    ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='medium')
    ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12, fontweight='medium')
    
    title = f'ROC Curve - {set_name}' if set_name else 'ROC Curve'
    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
    
    ax.legend(loc='lower right', frameon=True, fancybox=True, shadow=True)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # Add performance summary
    summary_text = (f'Optimal threshold: {optimal_threshold:.3f}\n'
                    f'Sensitivity: {tpr[optimal_idx]:.3f}\n'
                    f'Specificity: {1-fpr[optimal_idx]:.3f}\n'
                    f'Youden index: {j_scores[optimal_idx]:.3f}')
    
    ax.text(0.6, 0.2, summary_text, transform=ax.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    return auc_val, optimal_threshold

def create_enhanced_confusion_matrix(y_true, y_pred, set_name="", ax=None):
    """
    Create enhanced confusion matrix
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6))
    
    cm = confusion_matrix(y_true, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Create heatmap
    im = ax.imshow(cm_normalized, interpolation='nearest', cmap=blue_cmap, vmin=0, vmax=1)
    
    # Add colorbar
    cbar = ax.figure.colorbar(im, ax=ax, shrink=0.8)
    cbar.ax.set_ylabel('Proportion', rotation=270, labelpad=20)
    
    # Set labels
    classes = ['Non-BA', 'BA']
    tick_marks = np.arange(len(classes))
    
    ax.set_xticks(tick_marks)
    ax.set_yticks(tick_marks)
    ax.set_xticklabels(classes, fontsize=11)
    ax.set_yticklabels(classes, fontsize=11)
    
    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='medium')
    ax.set_ylabel('True Label', fontsize=12, fontweight='medium')
    
    title = f'Confusion Matrix - {set_name}' if set_name else 'Confusion Matrix'
    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
    
    # Add text annotations
    thresh = cm_normalized.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(j, i-0.1, f'{cm[i, j]}',
                ha="center", va="center",
                color="white" if cm_normalized[i, j] > thresh else "black",
                fontsize=12, fontweight='bold')
        
        ax.text(j, i+0.1, f'({cm_normalized[i, j]:.1%})',
                ha="center", va="center",
                color="white" if cm_normalized[i, j] > thresh else "black",
                fontsize=10)
    
    # Calculate performance metrics
    accuracy = np.trace(cm) / np.sum(cm)
    sensitivity = cm[1,1] / cm[1,:].sum() if cm[1,:].sum() > 0 else 0
    specificity = cm[0,0] / cm[0,:].sum() if cm[0,:].sum() > 0 else 0
    ppv = cm[1,1] / cm[:,1].sum() if cm[:,1].sum() > 0 else 0
    npv = cm[0,0] / cm[:,0].sum() if cm[:,0].sum() > 0 else 0
    
    # Add metrics box
    metrics_text = (f'Accuracy: {accuracy:.3f}\n'
                    f'Sensitivity: {sensitivity:.3f}\n'
                    f'Specificity: {specificity:.3f}\n'
                    f'PPV: {ppv:.3f}\n'
                    f'NPV: {npv:.3f}')
    
    ax.text(1.7, 0.5, metrics_text, transform=ax.transAxes,
            fontsize=9, verticalalignment='center',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    return cm

def create_probability_distribution(y_true, y_proba, set_name="", ax=None):
    """
    Create probability distribution histogram
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))
    
    # Separate BA and non-BA probabilities
    ba_probs = y_proba[y_true == 1]
    non_ba_probs = y_proba[y_true == 0]
    
    # Create histogram
    bins = np.linspace(0, 1, 31)
    
    ax.hist(non_ba_probs, bins=bins, alpha=0.7, label='Non-BA',
            color=CUSTOM_COLORS['primary'], edgecolor='black', density=True)
    ax.hist(ba_probs, bins=bins, alpha=0.7, label='BA',
            color=CUSTOM_COLORS['accent'], edgecolor='black', density=True)
    
    # Add decision threshold line
    ax.axvline(0.5, color='red', linestyle='--', linewidth=2, alpha=0.8, label='Threshold (0.5)')
    
    # Add probability density curves
    if len(ba_probs) > 1:
        kde_ba = gaussian_kde(ba_probs)
        x_ba = np.linspace(0, 1, 100)
        ax.plot(x_ba, kde_ba(x_ba), color='darkred', linewidth=2, alpha=0.8)
    
    if len(non_ba_probs) > 1:
        kde_non_ba = gaussian_kde(non_ba_probs)
        x_non_ba = np.linspace(0, 1, 100)
        ax.plot(x_non_ba, kde_non_ba(x_non_ba), color='darkblue', linewidth=2, alpha=0.8)
    
    ax.set_xlabel('Predicted Probability', fontsize=12, fontweight='medium')
    ax.set_ylabel('Density', fontsize=12, fontweight='medium')
    
    title = f'Probability Distribution - {set_name}' if set_name else 'Probability Distribution'
    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
    
    ax.legend(frameon=True, fancybox=True, shadow=True)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # Add statistics
    stats_text = (f'Non-BA (n={len(non_ba_probs)})\n'
                  f'  Mean: {non_ba_probs.mean():.3f}\n'
                  f'  SD: {non_ba_probs.std():.3f}\n\n'
                  f'BA (n={len(ba_probs)})\n'
                  f'  Mean: {ba_probs.mean():.3f}\n'
                  f'  SD: {ba_probs.std():.3f}')
    
    ax.text(0.72, 0.95, stats_text, transform=ax.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    return ax

def create_calibration_curve(y_true, y_proba, set_name="", ax=None):
    """
    Create calibration curve
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6))
    
    prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10, strategy='quantile')
    
    # Create scatter plot with lines
    ax.plot(prob_pred, prob_true, 'o-', linewidth=2.5, markersize=8,
            color=CUSTOM_COLORS['primary'], label='Model calibration')
    
    # Perfect calibration line
    ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5, label='Perfect calibration')
    
    # Fill areas
    ax.fill_between(prob_pred, prob_pred, prob_true, where=(prob_true >= prob_pred),
                    alpha=0.2, color=CUSTOM_COLORS['accent'], label='Overconfident')
    ax.fill_between(prob_pred, prob_pred, prob_true, where=(prob_true < prob_pred),
                    alpha=0.2, color=CUSTOM_COLORS['success'], label='Underconfident')
    
    # Add histogram showing sample distribution
    ax2 = ax.twinx()
    bins = np.linspace(0, 1, 11)
    bin_indices = np.digitize(y_proba, bins) - 1
    bin_counts = [np.sum(bin_indices == i) for i in range(10)]
    bin_centers = (bins[:-1] + bins[1:]) / 2
    
    ax2.bar(bin_centers, bin_counts, width=0.08, alpha=0.3,
            color=CUSTOM_COLORS['gray1'], label='Sample count')
    ax2.set_ylabel('Sample count', fontsize=10)
    ax2.set_ylim(0, max(bin_counts) * 1.3)
    
    ax.set_xlim([-0.05, 1.05])
    ax.set_ylim([-0.05, 1.05])
    ax.set_xlabel('Mean Predicted Probability', fontsize=12, fontweight='medium')
    ax.set_ylabel('Fraction of Positives', fontsize=12, fontweight='medium')
    
    title = f'Calibration Curve - {set_name}' if set_name else 'Calibration Curve'
    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)
    
    # Merge legends
    lines1, labels1 = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
    
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # Calculate Brier score
    brier = brier_score_loss(y_true, y_proba)
    ax.text(0.05, 0.95, f'Brier score: {brier:.4f}',
            transform=ax.transAxes, fontsize=10,
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    return brier

print("âœ… Enhanced visualization functions loaded")

# ====================
# è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
# ====================
def train_random_forest_model():
    print("\n" + "="*60)
    print("ğŸŒ² Training Random Forest Model for Neonatal BA Diagnosis")
    print("="*60)
    
    # ä½¿ç”¨ä¸notebookç›¸åŒçš„RFå‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    rf_model = RandomForestClassifier(
        n_estimators=100,        # æ ‘çš„æ•°é‡
        max_depth=5,            # é™åˆ¶æ ‘æ·±åº¦
        min_samples_split=10,   # åˆ†å‰²æ‰€éœ€æœ€å°æ ·æœ¬æ•°
        min_samples_leaf=5,     # å¶èŠ‚ç‚¹æ‰€éœ€æœ€å°æ ·æœ¬æ•°
        max_features=0.5,       # æ¯æ£µæ ‘ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹
        bootstrap=True,
        random_state=42,
        n_jobs=-1,
        verbose=0
    )
    
    print("\nğŸš€ Training model...")
    rf_model.fit(class_x_train, class_y_train)
    
    return rf_model

# ====================
# æ¨¡å‹è¯„ä¼°å‡½æ•°
# ====================
def evaluate_model(model, X_train, y_train, X_test, y_test, model_name="Random Forest"):
    """
    è¯„ä¼°æ¨¡å‹å¹¶ç”Ÿæˆå¯è§†åŒ–
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ {model_name} - Performance Evaluation")
    print('='*60)
    
    # è®­ç»ƒé›†é¢„æµ‹
    y_train_pred = model.predict(X_train)
    y_train_proba = model.predict_proba(X_train)[:, 1]
    
    # æµ‹è¯•é›†é¢„æµ‹
    y_test_pred = model.predict(X_test)
    y_test_proba = model.predict_proba(X_test)[:, 1]
    
    # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
    train_metrics = {
        'train_Accuracy': accuracy_score(y_train, y_train_pred),
        'train_Precision': precision_score(y_train, y_train_pred),
        'train_Recall': recall_score(y_train, y_train_pred),
        'train_F1': f1_score(y_train, y_train_pred),
        'train_AUC': roc_auc_score(y_train, y_train_proba),
        'train_MCC': matthews_corrcoef(y_train, y_train_pred)
    }
    
    # è®¡ç®—ç‰¹å¼‚æ€§
    cm_train = confusion_matrix(y_train, y_train_pred)
    if cm_train.shape == (2, 2):
        train_metrics['train_Specificity'] = cm_train[0,0] / cm_train[0,:].sum() if cm_train[0,:].sum() > 0 else 0
    
    # è®¡ç®—æµ‹è¯•é›†æŒ‡æ ‡
    test_metrics = {
        'Accuracy': accuracy_score(y_test, y_test_pred),
        'Precision': precision_score(y_test, y_test_pred),
        'Recall': recall_score(y_test, y_test_pred),
        'F1': f1_score(y_test, y_test_pred),
        'AUC': roc_auc_score(y_test, y_test_proba),
        'MCC': matthews_corrcoef(y_test, y_test_pred)
    }
    
    # è®¡ç®—ç‰¹å¼‚æ€§
    cm_test = confusion_matrix(y_test, y_test_pred)
    if cm_test.shape == (2, 2):
        test_metrics['Specificity'] = cm_test[0,0] / cm_test[0,:].sum() if cm_test[0,:].sum() > 0 else 0
    
    # AUCç½®ä¿¡åŒºé—´
    train_auc, train_auc_ci = calculate_auc_ci(y_train, y_train_proba)
    test_auc, test_auc_ci = calculate_auc_ci(y_test, y_test_proba)
    
    train_metrics['train_AUC_CI'] = train_auc_ci
    test_metrics['AUC_CI'] = test_auc_ci
    
    # ç»„åˆæŒ‡æ ‡
    combined_metrics = {**train_metrics, **test_metrics}
    
    # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒDataFrame
    metrics_df = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', 'Specificity', 'MCC'],
        'Training': [
            f"{train_metrics['train_Accuracy']:.4f}",
            f"{train_metrics['train_Precision']:.4f}",
            f"{train_metrics['train_Recall']:.4f}",
            f"{train_metrics['train_F1']:.4f}",
            f"{train_metrics['train_AUC']:.4f}",
            f"{train_metrics.get('train_Specificity', 0):.4f}",
            f"{train_metrics['train_MCC']:.4f}"
        ],
        'Test': [
            f"{test_metrics['Accuracy']:.4f}",
            f"{test_metrics['Precision']:.4f}",
            f"{test_metrics['Recall']:.4f}",
            f"{test_metrics['F1']:.4f}",
            f"{test_metrics['AUC']:.4f}",
            f"{test_metrics.get('Specificity', 0):.4f}",
            f"{test_metrics['MCC']:.4f}"
        ]
    })
    
    print("\nğŸ“Š Performance Metrics Comparison (Training vs Test):")
    print(metrics_df.to_string(index=False))
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ Detailed Classification Report (Test Set):")
    print(classification_report(y_test, y_test_pred, target_names=['Non-BA', 'BA']))
    
    # åˆ›å»ºå¯è§†åŒ–
    print("\nğŸ¨ Generating comprehensive visualizations...")
    visualize_comprehensive_results(y_train, y_train_proba, y_train_pred, 
                                   y_test, y_test_proba, y_test_pred, model_name)
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)
    j_scores = tpr - fpr
    optimal_idx = np.argmax(j_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"\nğŸ¯ ROC-based Optimal Threshold: {optimal_threshold:.3f}")
    print(f"  Sensitivity at optimal threshold: {tpr[optimal_idx]:.3f}")
    print(f"  Specificity at optimal threshold: {1-fpr[optimal_idx]:.3f}")
    
    combined_metrics['optimal_threshold'] = optimal_threshold
    
    return combined_metrics

def visualize_comprehensive_results(y_train, y_train_proba, y_train_pred,
                                  y_test, y_test_proba, y_test_pred, model_name):
    """
    åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨
    """
    fig, axes = plt.subplots(3, 2, figsize=(16, 18))
    
    # 1. ROCæ›²çº¿ï¼ˆè®­ç»ƒé›†ï¼‰
    create_modern_roc_curve(y_train, y_train_proba, "Training Set", ax=axes[0, 0])
    
    # 2. ROCæ›²çº¿ï¼ˆæµ‹è¯•é›†ï¼‰
    create_modern_roc_curve(y_test, y_test_proba, "Test Set", ax=axes[0, 1])
    
    # 3. æ··æ·†çŸ©é˜µï¼ˆè®­ç»ƒé›†ï¼‰
    create_enhanced_confusion_matrix(y_train, y_train_pred, "Training Set", ax=axes[1, 0])
    
    # 4. æ··æ·†çŸ©é˜µï¼ˆæµ‹è¯•é›†ï¼‰
    create_enhanced_confusion_matrix(y_test, y_test_pred, "Test Set", ax=axes[1, 1])
    
    # 5. æ¦‚ç‡åˆ†å¸ƒ
    create_probability_distribution(y_test, y_test_proba, "Test Set", ax=axes[2, 0])
    
    # 6. æ ¡å‡†æ›²çº¿
    create_calibration_curve(y_test, y_test_proba, "Test Set", ax=axes[2, 1])
    
    plt.suptitle(f'{model_name} - Comprehensive Analysis', fontsize=20, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºPDF
    pdf_path = f'random_forest_comprehensive_analysis.pdf'
    plt.savefig(pdf_path, format='pdf', bbox_inches='tight', dpi=300)
    print(f"  ğŸ“„ PDF saved: {pdf_path}")
    
    plt.show()

# ====================
# ç‰¹å¾é‡è¦æ€§åˆ†æ
# ====================
def analyze_feature_importance(model, feature_names):
    """
    åˆ†æç‰¹å¾é‡è¦æ€§
    """
    print("\nğŸ“Š Feature Importance Analysis")
    print(f"{'='*60}")
    
    # è·å–ç‰¹å¾é‡è¦æ€§
    rf_importance = model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': rf_importance
    }).sort_values('Importance', ascending=False)
    
    print("Random Forest Feature Importance:")
    print(feature_importance_df.to_string(index=False))
    
    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    plt.figure(figsize=(10, 6))
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_names)))
    bars = plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color=colors)
    plt.xlabel('Importance (Gini Index)', fontsize=12, fontweight='medium')
    plt.ylabel('Feature', fontsize=12, fontweight='medium')
    plt.title('Random Forest Feature Importance', fontsize=14, fontweight='bold')
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (v, bar) in enumerate(zip(feature_importance_df['Importance'], bars)):
        plt.text(v + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{v:.3f}', ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('feature_importance.pdf', format='pdf', bbox_inches='tight', dpi=300)
    plt.show()
    
    return feature_importance_df

# ====================
# äº¤å‰éªŒè¯
# ====================
def perform_cross_validation(model, X, y, n_splits=5):
    """
    æ‰§è¡Œäº¤å‰éªŒè¯
    """
    print("\nğŸ”¬ Cross-Validation Validation")
    print(f"{'='*60}")
    
    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # è®¡ç®—AUCäº¤å‰éªŒè¯åˆ†æ•°
    cv_auc = cross_val_score(
        model, 
        X, 
        y,
        cv=cv,
        scoring='roc_auc',
        n_jobs=-1
    )
    
    print(f"Cross-validation AUC: {cv_auc.mean():.4f} Â± {cv_auc.std():.4f}")
    print(f"Individual fold AUCs: {cv_auc}")
    
    # å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(range(1, n_splits+1), cv_auc, color=CUSTOM_COLORS['primary'], alpha=0.7)
    ax.axhline(y=cv_auc.mean(), color='red', linestyle='--', linewidth=2, 
               label=f'Mean AUC = {cv_auc.mean():.3f}')
    ax.set_xlabel('Fold', fontsize=12)
    ax.set_ylabel('AUC', fontsize=12)
    ax.set_title(f'{n_splits}-Fold Cross-Validation Results', fontsize=14, fontweight='bold')
    ax.set_xticks(range(1, n_splits+1))
    ax.set_ylim([0, 1.05])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('cross_validation_results.pdf', format='pdf', bbox_inches='tight', dpi=300)
    plt.show()
    
    return cv_auc

# ====================
# ä¸»è®­ç»ƒå‡½æ•°
# ====================
def main():
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    np.random.seed(42)
    
    # å¿½ç•¥è­¦å‘Š
    warnings.filterwarnings('ignore')
    
    print("\n" + "="*60)
    print("ğŸš€ Neonatal Biliary Atresia Random Forest Model")
    print("="*60)
    
    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    rf_model = train_random_forest_model()
    
    # è¯„ä¼°æ¨¡å‹
    feature_names = class_x_train.columns.tolist()
    metrics = evaluate_model(rf_model, class_x_train, class_y_train, 
                            class_x_test, class_y_test, "Random Forest")
    
    # åˆ†æç‰¹å¾é‡è¦æ€§
    feature_importance_df = analyze_feature_importance(rf_model, feature_names)
    
    # æ‰§è¡Œäº¤å‰éªŒè¯
    cv_scores = perform_cross_validation(rf_model, data_scaled, data_target)
    
    # æ£€æŸ¥è¿‡æ‹Ÿåˆ
    print("\nğŸ” Overfitting Check:")
    train_acc = metrics['train_Accuracy']
    test_acc = metrics['Accuracy']
    if train_acc > 0.95 and test_acc < 0.85:
        print("âš ï¸ Warning: Model may be overfitting!")
        print(f"  Training accuracy: {train_acc:.3f}")
        print(f"  Test accuracy: {test_acc:.3f}")
        print(f"  Difference: {train_acc - test_acc:.3f}")
    else:
        print("âœ… Model generalization performance is good")
    
    # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
    joblib.dump(rf_model, 'neonatal_ba_rf_model.pkl')
    joblib.dump(scaler, 'neonatal_ba_scaler.pkl')
    print("\nâœ… Model and scaler saved successfully")
    
    # ä¿å­˜ç‰¹å¾ä¿¡æ¯
    feature_info = {
        'features': feature_names,
        'feature_count': len(feature_names),
        'feature_descriptions': {
            'GB_length': 'Gallbladder Length (mm)',
            'Abnormal_GEI': 'Abnormal Gallbladder Emptying Index (Binary)',
            'GGT': 'Gamma-Glutamyl Transferase (U/L)',
            'DBIL': 'Direct Bilirubin (Î¼mol/L)',
            'MMP7': 'Matrix Metalloproteinase-7 (ng/mL)'
        }
    }
    joblib.dump(feature_info, 'feature_info.pkl')
    print("âœ… Feature information saved")
    
    # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
    performance_data = {
        'model': 'RandomForestClassifier',
        'metrics': metrics,
        'feature_importance': feature_importance_df.to_dict('records'),
        'cv_scores': cv_scores.tolist(),
        'cv_mean': float(cv_scores.mean()),
        'cv_std': float(cv_scores.std()),
        'dataset_size': data.shape[0],
        'train_size': class_x_train.shape[0],
        'test_size': class_x_test.shape[0],
        'feature_names': feature_names
    }
    joblib.dump(performance_data, 'performance_metrics.pkl')
    
    # åˆ›å»ºæ€§èƒ½æ€»ç»“CSVæ–‡ä»¶
    summary_df = pd.DataFrame({
        'Metric': ['AUC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'F1', 'MCC'],
        'Training': [
            metrics['train_AUC'],
            metrics['train_Accuracy'],
            metrics['train_Precision'],
            metrics['train_Recall'],
            metrics.get('train_Specificity', 0),
            metrics['train_F1'],
            metrics['train_MCC']
        ],
        'Test': [
            metrics['AUC'],
            metrics['Accuracy'],
            metrics['Precision'],
            metrics['Recall'],
            metrics.get('Specificity', 0),
            metrics['F1'],
            metrics['MCC']
        ]
    })
    summary_df.to_csv('model_performance_summary.csv', index=False)
    
    print("âœ… Performance metrics saved")
    
    # ä¸´åºŠå»ºè®®
    print("\n" + "="*60)
    print("ğŸ¥ Clinical Application Recommendations")
    print("="*60)
    
    if feature_importance_df is not None:
        print("\nğŸ“‹ Top 3 Predictive Features:")
        for i, (feature, importance) in enumerate(feature_importance_df.head(3).values):
            print(f"   {i+1}. {feature}: Importance={importance:.3f}")
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    y_test_proba = rf_model.predict_proba(class_x_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(class_y_test, y_test_proba)
    j_scores = tpr - fpr
    optimal_idx = np.argmax(j_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"\nğŸ¯ Recommended Decision Threshold: {optimal_threshold:.3f}")
    print(f"  Sensitivity at this threshold: {tpr[optimal_idx]:.3f}")
    print(f"  Specificity at this threshold: {1-fpr[optimal_idx]:.3f}")
    
    # ä¿å­˜ä¸´åºŠå»ºè®®
    clinical_recommendations = {
        'optimal_threshold': optimal_threshold,
        'sensitivity': tpr[optimal_idx],
        'specificity': 1-fpr[optimal_idx],
        'top_feature_1': feature_importance_df.iloc[0]['Feature'] if feature_importance_df is not None else '',
        'top_feature_1_importance': feature_importance_df.iloc[0]['Importance'] if feature_importance_df is not None else 0,
        'top_feature_2': feature_importance_df.iloc[1]['Feature'] if feature_importance_df is not None else '',
        'top_feature_2_importance': feature_importance_df.iloc[1]['Importance'] if feature_importance_df is not None else 0,
        'top_feature_3': feature_importance_df.iloc[2]['Feature'] if feature_importance_df is not None else '',
        'top_feature_3_importance': feature_importance_df.iloc[2]['Importance'] if feature_importance_df is not None else 0
    }
    
    clinical_df = pd.DataFrame([clinical_recommendations])
    clinical_df.to_csv('clinical_recommendations.csv', index=False)
    print("âœ… Clinical recommendations saved")
    
    # æœ€ç»ˆè¾“å‡º
    print("\n" + "="*60)
    print("ğŸ‰ Training completed successfully!")
    print("="*60)
    print("\nğŸ“ Generated files:")
    print("  - neonatal_ba_rf_model.pkl (Random Forest model)")
    print("  - neonatal_ba_scaler.pkl (Feature scaler)")
    print("  - feature_info.pkl (Feature information)")
    print("  - performance_metrics.pkl (Performance metrics)")
    print("  - model_performance_summary.csv (Performance summary)")
    print("  - clinical_recommendations.csv (Clinical recommendations)")
    print("  - random_forest_comprehensive_analysis.pdf (Comprehensive visualization)")
    print("  - feature_importance.pdf (Feature importance plot)")
    print("  - cross_validation_results.pdf (Cross-validation results)")
    
    return rf_model, scaler, metrics, feature_names

if __name__ == "__main__":
    try:
        model, scaler, metrics, feature_names = main()
    except Exception as e:
        print(f"\nâŒ Error during training: {str(e)}")
        import traceback
        traceback.print_exc()